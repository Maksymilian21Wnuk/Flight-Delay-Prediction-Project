{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1OjFyfzJpUD2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import opendatasets as od\n",
    "from joblib import parallel_backend\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as sopt\n",
    "import scipy.stats as sstats\n",
    "import seaborn as sns\n",
    "import sklearn.ensemble\n",
    "import sklearn.tree\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import animation, pyplot, rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from datetime import datetime, time\n",
    "import json \n",
    "\n",
    "PLOT_MODULE = \"/home/meks/Desktop/Flight-Delay-Prediction-Project/ml/\"\n",
    "sys.path.append(PLOT_MODULE)\n",
    "\n",
    "from EDA_plottings import run_plots\n",
    "\n",
    "# garbage collector so we can save memory\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJr43qqJemQ6"
   },
   "source": [
    "# Problem Statement and Objectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnBQtMZ6pCLx"
   },
   "source": [
    "# Data Colection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge7ygca0rWfx"
   },
   "source": [
    "## Importing preprocessed data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSk87wlvwdd5"
   },
   "source": [
    "We will be importing data that was merged data of flight dataset from kaggle\n",
    "and weather from visual crossing (https://www.visualcrossing.com) page. Thanks for cooperation with them,\n",
    "we were allowed to make large queries about weather in certain places and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6UynddHrHyl",
    "outputId": "c1509dda-867d-4c44-dca5-0a220235cbdf"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/home/meks/Desktop/data_cale.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru3JuHhwwvee"
   },
   "source": [
    "Lets open it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vUWe7_e0uFiZ"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#NOT SURE ABOUT CRS DEST DISTANCE\n",
    "dataset = pd.read_csv(DATASET_PATH, dtype={'snow':'uint16', 'rain':'uint16','freezingrain':'uint16','ice':'uint16',\n",
    "                                                 'FL_MONTH':'uint16', 'FL_DAY' : 'uint16', 'FL_YEAR':'uint32', 'OP_CARRIER':'uint16',\n",
    "                                           'DELAY' : 'uint16', 'DEST' : 'uint16', 'CRS_ARR_TIME' : 'uint16', 'DISTANCE' : 'uint16'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "lVgnvkNkvto9",
    "outputId": "bd7396f8-5d26-49c5-b012-a18520fe8354"
   },
   "outputs": [],
   "source": [
    "dataset.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1Yt0aScxdha",
    "outputId": "7b44a078-2768-42ff-fb0e-fd12b145661c"
   },
   "outputs": [],
   "source": [
    "dataset.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azQB8C5q6J01"
   },
   "source": [
    "# A small peek on size of data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0XpNt4QFdgG"
   },
   "source": [
    "It's definetely too much, we need to make it smaller, especially the string columns. We would like to convert big string data into mappings of ints to strings. Lets look at example of big columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsaEVldv62MF",
    "outputId": "9bb74ba9-d950-4b9f-bbac-81a786495d32"
   },
   "outputs": [],
   "source": [
    "## do wyjebania imo  AIRLINE_DOT,AIRLINE_DOT,DOT_CODE,ORIGIN,DEST ewentualnie zostawic skroty i zrobic jakis slownik do wypisywania\n",
    "for col_name in dataset.columns.values:\n",
    "  print(f\"Size in mb of {col_name} : {sys.getsizeof(dataset[col_name]) / 10 ** 6} mb, Type {dataset[col_name].dtype}\")\n",
    "print(f\"Data size : {sys.getsizeof(dataset) / 10 ** 9} in GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gibK5EKYhyO9"
   },
   "source": [
    "# Data Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egv0ODfVn6Uj"
   },
   "source": [
    "Info about columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX1ldSNQehFW",
    "outputId": "607805a7-5c06-43dc-9bdf-c906e857ba7f"
   },
   "outputs": [],
   "source": [
    "print(f\"There ara {dataset.shape[0]} rows and {dataset.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OO79O3q9igyf",
    "outputId": "3ad77bd4-0ffb-4a3b-84e4-5c47d8ad4af2"
   },
   "outputs": [],
   "source": [
    "dataset.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiW2YjHPk21k",
    "outputId": "28c48b53-4324-4775-bf33-930c777caa7e"
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKIcQWp4g7ll"
   },
   "source": [
    "Mean of delays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpAJaoEEg-3N",
    "outputId": "1e01bd49-b9d6-4885-d5c1-313fef8f6e7b"
   },
   "outputs": [],
   "source": [
    "delays = dataset['DELAY']\n",
    "delays.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many nans in dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should remove windgust? So many NaNs, probably no colleration, ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kroz9iZPIhUx"
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_nans():\n",
    "  # since all len's are same\n",
    "  n = len(dataset['DELAY'])\n",
    "  for col in dataset.columns.values:\n",
    "    nans = dataset[col].isna().sum()\n",
    "    print(f\"Col name: {col} nans: {nans}   {(nans / n) * 100} %\")\n",
    "  print(f\"There are {n} rows\")\n",
    "\n",
    "check_nans()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for some plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pie graph of delays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wykreskolowydelay(df):\n",
    "    delayed_flights = df[df['DELAY'] > 0]\n",
    "    percentages = [len(delayed_flights) / len(df) * 100, 100 - len(delayed_flights) / len(df) * 100]\n",
    "    labels = ['Delayed', 'Non-delayed']\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(percentages, labels=labels, autopct='%1.1f%%', colors=['skyblue', 'lightgreen'], startangle=90)\n",
    "    plt.title('Percentage of delayed')\n",
    "    plt.show()\n",
    "\n",
    "def wykresmiesiacdealay(df):\n",
    "    monthly_delays = df.groupby(df['FL_MONTH'])['DELAY'].mean() * 100\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title('Procentowe opóźnienia')\n",
    "    plt.xlabel('Miesiąc')\n",
    "    plt.ylabel('Procentowe opóźnienie')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "def wykresmiesiacliczbalotow(df):\n",
    "    monthly_delays = df.groupby(df['FL_MONTH'])['id'].count()\n",
    "    print(monthly_delays)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title('Liczba lotów na miesiąc')\n",
    "    plt.xlabel('Miesiąc')\n",
    "    plt.ylabel('Liczba lotów')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "def wykresmiesiacopoznienie(df):\n",
    "    monthly_delays = df.groupby('FL_MONTH')['DELAY'].sum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    print(monthly_delays)\n",
    "    monthly_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title('Ilość opóźnionych lotów')\n",
    "    plt.xlabel('Miesiąc')\n",
    "    plt.ylabel('Liczba opóźnień')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "graphs = [wykreskolowydelay, wykresmiesiacdealay, wykresmiesiacliczbalotow, wykresmiesiacopoznienie]\n",
    "\n",
    "for g in graphs:\n",
    "    g(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ud6a5QJDh3cb"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4rgBEdYkvzT"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY2S_PtTkybL"
   },
   "source": [
    "Let's remove windgust, since it doesn't provide any important data (does it?) and contains of lots of nans.\n",
    "Also, 8 percents of snow might be too much. For now, we will remove it and maybe later on we will return it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_g_Y_phSm-0N"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['WINDGUST', 'SNOW', \"SNOWDEPTH\", \"ID\", \"id\"], axis = 1, errors = 'ignore')\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAcEdCpyMk7K"
   },
   "source": [
    "Now it would be great if we removed NaN rows, I will remove all NaN rows from MOONPHASE, CONDITIONS etc. I know only by seeing those rows, that there since moonphase and conditions and cloudcover etc. have 0.002481972697193719 % of nans, they have nans on all rows. So i need to remove only moonphase visibility and pressure rows with NaN's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ms4Nt3oxM2My"
   },
   "outputs": [],
   "source": [
    "nan_cols = [\"MOONPHASE\", 'VISIBILITY', 'PRESSURE']\n",
    "\n",
    "dataset.dropna(subset = nan_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dkX4TztPoww"
   },
   "source": [
    "Now lets look at nans in overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hj5rtsQyPsgC"
   },
   "outputs": [],
   "source": [
    "check_nans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wm9yLL7qyLSj"
   },
   "source": [
    "We have our data cleaned. Data we imported had ~12.65 mln. rows, after cleaning we have ~12.64 mln. rows. (???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSsljOVBlBGd"
   },
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AF4bNmClHh_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUN-ZZpwlIsh"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjEC1L0XlNpO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIHFTog_lOvl"
   },
   "source": [
    "## Tranformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzhFvauQlUiU"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5QgLbmrp1NJ"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0tvemvfp6PB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98ZnWfqHnYpg"
   },
   "source": [
    "# Machine Learning Proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIvR1vYRnb2A"
   },
   "outputs": [],
   "source": [
    "def LogREG(X_train,X_test,y_train,y_test):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Different logistic regression parameters to try\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag']}\n",
    "\n",
    "    # Use GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and best model from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(\"LR best parameters:\", best_params)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "def SVM(X_train, X_test, y_train, y_test):\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Dokładność klasyfikacji: {accuracy:.2f}')\n",
    "\n",
    "    # Wyświetlanie pełnego raportu klasyfikacji\n",
    "    print('\\nRaport klasyfikacji:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "def DecisionTree(X_train, X_test, y_train, y_test, max_depth=4):\n",
    "    # Initializing the decision tree classifier\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42, criterion='gini')\n",
    "\n",
    "    # Training the classifier on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting labels for the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluating the classifier's accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Classification Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    # Displaying the full classification report\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plotting the decision tree\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_tree(clf, filled=True, feature_names=X_train.columns, class_names=['Class 0', 'Class 1'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.loc[:, dataset.columns != \"DELAY\"]\n",
    "y = dataset[\"DELAY\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogREG(X_train, X_test, y_train, y_test)\n",
    "#SVM(X_train, X_test, y_train, y_test)\n",
    "DecisionTree(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GSsljOVBlBGd",
    "LUN-ZZpwlIsh",
    "k5QgLbmrp1NJ",
    "98ZnWfqHnYpg"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
